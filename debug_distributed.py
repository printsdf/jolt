#!/usr/bin/env python3
"""
åˆ†å¸ƒå¼è®­ç»ƒè°ƒè¯•è„šæœ¬
ç”¨äºè¯Šæ–­JOLTé¡¹ç›®ä¸­çš„ChildFailedErroré—®é¢˜
"""

import torch
import os
import sys
from pathlib import Path

def check_cuda_setup():
    """æ£€æŸ¥CUDAè®¾ç½®"""
    print("=== CUDA è®¾ç½®æ£€æŸ¥ ===")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            print(f"GPU {i}: {props.name}")
            print(f"  æ˜¾å­˜: {props.total_memory / 1024**3:.1f} GB")
            print(f"  è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
            
            # æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
            torch.cuda.set_device(i)
            allocated = torch.cuda.memory_allocated(i) / 1024**3
            cached = torch.cuda.memory_reserved(i) / 1024**3
            print(f"  å·²åˆ†é…æ˜¾å­˜: {allocated:.1f} GB")
            print(f"  ç¼“å­˜æ˜¾å­˜: {cached:.1f} GB")
    else:
        print("CUDAä¸å¯ç”¨!")
        return False
    
    return True

def check_model_path():
    """æ£€æŸ¥æ¨¡å‹è·¯å¾„"""
    print("\n=== æ¨¡å‹è·¯å¾„æ£€æŸ¥ ===")
    model_path = "/data/models/Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4"
    
    if os.path.exists(model_path):
        print(f"âœ“ æ¨¡å‹è·¯å¾„å­˜åœ¨: {model_path}")
        
        # æ£€æŸ¥å…³é”®æ–‡ä»¶
        key_files = ["config.json", "tokenizer.json", "tokenizer_config.json"]
        for file in key_files:
            file_path = os.path.join(model_path, file)
            if os.path.exists(file_path):
                print(f"âœ“ {file} å­˜åœ¨")
            else:
                print(f"âœ— {file} ç¼ºå¤±")
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
        model_files = [f for f in os.listdir(model_path) if f.endswith(('.bin', '.safetensors', '.pt'))]
        if model_files:
            print(f"âœ“ æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_files}")
        else:
            print("âœ— æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
            
    else:
        print(f"âœ— æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return False
    
    return True

def check_data_file():
    """æ£€æŸ¥æ•°æ®æ–‡ä»¶"""
    print("\n=== æ•°æ®æ–‡ä»¶æ£€æŸ¥ ===")
    data_file = "data/nox_prediction.csv"
    
    if os.path.exists(data_file):
        print(f"âœ“ æ•°æ®æ–‡ä»¶å­˜åœ¨: {data_file}")
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        size = os.path.getsize(data_file) / 1024
        print(f"  æ–‡ä»¶å¤§å°: {size:.1f} KB")
        
        # æ£€æŸ¥å‰å‡ è¡Œ
        try:
            with open(data_file, 'r') as f:
                lines = f.readlines()[:5]
                print(f"  æ€»è¡Œæ•°: {len(lines)} (æ˜¾ç¤ºå‰5è¡Œ)")
                for i, line in enumerate(lines):
                    print(f"    {i+1}: {line.strip()[:100]}...")
        except Exception as e:
            print(f"  è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            
    else:
        print(f"âœ— æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        return False
    
    return True

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    print("\n=== æ¨¡å‹åŠ è½½æµ‹è¯• ===")
    
    try:
        from hf_api import get_model_and_tokenizer
        from parse_args import init_option_parser
        
        # åˆ›å»ºæµ‹è¯•å‚æ•°
        parser = init_option_parser()
        test_args = parser.parse_args([
            "--llm_type", "qwen2.5-7B-instruct",
            "--llm_path", "/data/models/Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4",
            "--batch_size", "1"
        ])
        
        print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        model, tokenizer = get_model_and_tokenizer(test_args)
        print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ!")
        
        # æµ‹è¯•ç®€å•æ¨ç†
        test_input = "Hello, how are you?"
        inputs = tokenizer(test_input, return_tensors="pt")
        
        if torch.cuda.is_available():
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=10)
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"âœ“ æ¨ç†æµ‹è¯•æˆåŠŸ: {response}")
        
        return True
        
    except Exception as e:
        print(f"âœ— æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def check_accelerate_config():
    """æ£€æŸ¥accelerateé…ç½®"""
    print("\n=== Accelerate é…ç½®æ£€æŸ¥ ===")
    
    try:
        from accelerate import Accelerator
        from accelerate.utils import gather_object
        
        accelerator = Accelerator()
        print(f"âœ“ Acceleratoråˆå§‹åŒ–æˆåŠŸ")
        print(f"  è®¾å¤‡: {accelerator.device}")
        print(f"  è¿›ç¨‹æ•°: {accelerator.num_processes}")
        print(f"  æœ¬åœ°è¿›ç¨‹æ•°: {accelerator.local_process_index}")
        print(f"  æ˜¯å¦ä¸»è¿›ç¨‹: {accelerator.is_main_process}")
        
        return True
        
    except Exception as e:
        print(f"âœ— Accelerateé…ç½®æœ‰é—®é¢˜: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("JOLT åˆ†å¸ƒå¼è®­ç»ƒè¯Šæ–­å·¥å…·")
    print("=" * 50)
    
    checks = [
        ("CUDAè®¾ç½®", check_cuda_setup),
        ("æ¨¡å‹è·¯å¾„", check_model_path),
        ("æ•°æ®æ–‡ä»¶", check_data_file),
        ("Accelerateé…ç½®", check_accelerate_config),
        ("æ¨¡å‹åŠ è½½", test_model_loading),
    ]
    
    results = {}
    for name, check_func in checks:
        try:
            results[name] = check_func()
        except Exception as e:
            print(f"âœ— {name}æ£€æŸ¥æ—¶å‡ºé”™: {e}")
            results[name] = False
    
    print("\n" + "=" * 50)
    print("è¯Šæ–­ç»“æœæ±‡æ€»:")
    
    all_passed = True
    for name, passed in results.items():
        status = "âœ“ é€šè¿‡" if passed else "âœ— å¤±è´¥"
        print(f"  {name}: {status}")
        if not passed:
            all_passed = False
    
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æ£€æŸ¥éƒ½é€šè¿‡äº†ï¼å¯ä»¥å°è¯•è¿è¡Œè®­ç»ƒã€‚")
    else:
        print("\nâš ï¸  å‘ç°é—®é¢˜ï¼Œè¯·æ ¹æ®ä¸Šè¿°ä¿¡æ¯è¿›è¡Œä¿®å¤ã€‚")
    
    print("\nå»ºè®®çš„è¿è¡Œæ–¹å¼:")
    print("1. å¦‚æœæ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼Œä½¿ç”¨: bash scripts/qwen.sh")
    print("2. å¦‚æœæœ‰åˆ†å¸ƒå¼é—®é¢˜ï¼Œä½¿ç”¨: bash scripts/qwen_single_gpu.sh")
    print("3. å¦‚æœæ¨¡å‹åŠ è½½æœ‰é—®é¢˜ï¼Œæ£€æŸ¥æ¨¡å‹è·¯å¾„å’Œæƒé™")

if __name__ == "__main__":
    main()
