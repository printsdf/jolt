#!/bin/bash

# =============================================================================
# NOxé¢„æµ‹è‡ªåŠ¨åŒ–è„šæœ¬ (6GPUç»ˆæä¼˜åŒ–ç‰ˆæœ¬)
# ä½¿ç”¨æ‰€æœ‰å¯èƒ½çš„æ˜¾å­˜ä¼˜åŒ–æŠ€æœ¯ï¼Œç¡®ä¿ä¸ä¼šOOM
# =============================================================================

# ç»ˆææ˜¾å­˜ä¼˜åŒ–ç¯å¢ƒå˜é‡
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:32
export CUDA_LAUNCH_BLOCKING=0
export TORCH_USE_CUDA_DSA=1
export CUDA_CACHE_DISABLE=1

# åˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ–
export TORCH_DISTRIBUTED_DEBUG=WARN
export NCCL_DEBUG=WARN
export NCCL_TIMEOUT=7200
export NCCL_IB_DISABLE=1
export NCCL_P2P_DISABLE=1
export NCCL_TREE_THRESHOLD=0
export NCCL_ALGO=Ring

echo "ğŸš€ å¯åŠ¨6GPUç»ˆæä¼˜åŒ–NOxé¢„æµ‹ä»»åŠ¡"
echo "========================================"

# --- ç»ˆæä¿å®ˆå‚æ•° ---
EXPERIMENT_NAME="nox_qwen2_7b_6gpu_ultimate"

LLM_TYPE="qwen2.5-7B-instruct"
LLM_PATH="/data/models/Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4"

# æåº¦ä¿å®ˆçš„å‚æ•°è®¾ç½®
BATCH_SIZE=1             # æ¯ä¸ªGPUåªå¤„ç†1ä¸ªæ ·æœ¬
TRAIN_SIZE_LIMIT=18      # æ€»å…±18ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œæ¯ä¸ªGPU 3ä¸ª
TEST_SIZE_LIMIT=6        # æ€»å…±6ä¸ªæµ‹è¯•æ ·æœ¬ï¼Œæ¯ä¸ªGPU 1ä¸ª
NUM_SAMPLES=6            # æ€»å…±6ä¸ªé‡‡æ ·ï¼Œæ¯ä¸ªGPU 1ä¸ª
MAX_LENGTH=10            # æçŸ­çš„ç”Ÿæˆé•¿åº¦

# æœ€ç®€åŒ–çš„Prefix
PREFIX="Predict NOx."

TARGET_COLUMN="target"
SEED=42
DATA_FILE="data/nox_prediction.csv"
OUTPUT_DIR="./output/nox_experiments"

# --- ç»ˆæé¢„è¿è¡Œä¼˜åŒ– ---
echo "ğŸ”§ è¿›è¡Œç»ˆæ6GPUé¢„è¿è¡Œä¼˜åŒ–..."

# æ€æ­»å¯èƒ½å ç”¨GPUçš„è¿›ç¨‹
echo "æ¸…ç†GPUè¿›ç¨‹..."
nvidia-smi --query-compute-apps=pid --format=csv,noheader,nounits | xargs -r kill -9 2>/dev/null || true

# é‡ç½®GPUçŠ¶æ€
nvidia-smi --gpu-reset -i 0,1,2,3,4,5 2>/dev/null || true

# æ˜¾ç¤ºåˆå§‹GPUçŠ¶æ€
echo "åˆå§‹GPUçŠ¶æ€:"
nvidia-smi --query-gpu=index,name,memory.total,memory.used,memory.free,utilization.gpu --format=csv

# ç»ˆææ˜¾å­˜æ¸…ç†
echo "æ‰§è¡Œç»ˆææ˜¾å­˜æ¸…ç†..."
python -c "
import torch
import gc
import os

# å¼ºåˆ¶åƒåœ¾å›æ”¶
gc.collect()

if torch.cuda.is_available():
    print(f'æ£€æµ‹åˆ° {torch.cuda.device_count()} ä¸ªGPU')
    
    # æ¸…ç†æ¯ä¸ªGPU
    for i in range(torch.cuda.device_count()):
        torch.cuda.set_device(i)
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
        
        # è®¾ç½®æ˜¾å­˜åˆ†é…é™åˆ¶
        torch.cuda.set_per_process_memory_fraction(0.75, device=i)
        
        print(f'GPU {i} æ¸…ç†å®Œæˆï¼Œè®¾ç½®æ˜¾å­˜é™åˆ¶ä¸º75%')
    
    print('æ‰€æœ‰GPUç»ˆææ¸…ç†å®Œæˆ')
else:
    print('æœªæ£€æµ‹åˆ°CUDAè®¾å¤‡')
"

# æ£€æŸ¥æ–‡ä»¶
if [ ! -f "$DATA_FILE" ]; then
    echo "âŒ é”™è¯¯: æ•°æ®æ–‡ä»¶ $DATA_FILE ä¸å­˜åœ¨!"
    exit 1
fi

if [ ! -d "$LLM_PATH" ]; then
    echo "âŒ é”™è¯¯: æ¨¡å‹è·¯å¾„ '$LLM_PATH' ä¸å­˜åœ¨!"
    exit 1
fi

mkdir -p "$OUTPUT_DIR"

echo "âœ… é¢„æ£€æŸ¥å®Œæˆ"

# --- 6GPUç»ˆæåˆ†å¸ƒå¼æ‰§è¡Œ ---
echo "ğŸš€ å¼€å§‹6GPUç»ˆæåˆ†å¸ƒå¼è®­ç»ƒ..."
echo "é…ç½®: æ¯GPU batch=1, è®­ç»ƒæ ·æœ¬=18, æµ‹è¯•æ ·æœ¬=6, ç”Ÿæˆé•¿åº¦=10"

# ä½¿ç”¨æœ€ä¿å®ˆçš„å¯åŠ¨å‚æ•°
accelerate launch \
  --config_file accelerate_config_6gpu.yaml \
  --main_process_port 29500 \
  --num_cpu_threads_per_process 2 \
  run_jolt_ultra_optimized.py \
  --experiment_name "$EXPERIMENT_NAME" \
  --data_path "$DATA_FILE" \
  --llm_type "$LLM_TYPE" \
  --llm_path "$LLM_PATH" \
  --output_dir "$OUTPUT_DIR" \
  --batch_size $BATCH_SIZE \
  --train_size_limit $TRAIN_SIZE_LIMIT \
  --test_size_limit $TEST_SIZE_LIMIT \
  --csv_split_option "fixed_indices" \
  --train_start_index 1000 \
  --train_end_index 1018 \
  --test_start_index 1018 \
  --test_end_index 1024 \
  --mode sample_logpy \
  --y_column_names "$TARGET_COLUMN" \
  --y_column_types numerical \
  --num_decimal_places_x 2 \
  --num_decimal_places_y 1 \
  --max_generated_length $MAX_LENGTH \
  --header_option headers_as_item_prefix \
  --test_fraction 0.2 \
  --seed $SEED \
  --num_samples $NUM_SAMPLES \
  --temperature 0.3 \
  --prefix "$PREFIX"

# æ£€æŸ¥æ‰§è¡Œç»“æœ
RESULT=$?

if [ $RESULT -eq 0 ]; then
    echo "========================================"
    echo "ğŸ‰ 6GPUç»ˆæä¼˜åŒ–NOxé¢„æµ‹ä»»åŠ¡å®Œæˆ!"
    echo "è¯¦ç»†ç»“æœæ–‡ä»¶ä¿å­˜åœ¨: $OUTPUT_DIR"
    
    # æ˜¾ç¤ºæˆåŠŸåçš„GPUçŠ¶æ€
    echo "ä»»åŠ¡å®ŒæˆåGPUçŠ¶æ€:"
    nvidia-smi --query-gpu=index,memory.used,memory.free,utilization.gpu --format=csv
    
else
    echo "========================================"
    echo "âŒ 6GPUç»ˆæä¼˜åŒ–é¢„æµ‹ä»»åŠ¡å¤±è´¥!"
    echo "é”™è¯¯ä»£ç : $RESULT"
    
    echo "å¤±è´¥æ—¶GPUçŠ¶æ€:"
    nvidia-smi
    
    echo "å°è¯•ç´§æ€¥æ˜¾å­˜æ¸…ç†..."
    python -c "
import torch
import gc
gc.collect()
if torch.cuda.is_available():
    for i in range(torch.cuda.device_count()):
        torch.cuda.set_device(i)
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
    print('ç´§æ€¥æ¸…ç†å®Œæˆ')
"
fi

# æœ€ç»ˆæ¸…ç†
echo "æ‰§è¡Œæœ€ç»ˆæ¸…ç†..."
python -c "
import torch
import gc
import os

gc.collect()
if torch.cuda.is_available():
    for i in range(torch.cuda.device_count()):
        torch.cuda.set_device(i)
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
        torch.cuda.reset_peak_memory_stats(i)
    print('æœ€ç»ˆæ¸…ç†å®Œæˆ')
"

# é€šçŸ¥å®Œæˆ
if [ $RESULT -eq 0 ]; then
    curl -s 'https://oapi.dingtalk.com/robot/send?access_token=979c5387ce734aea95b5b368629c8a412284dc2f52674dd0291eab5f154fb70a' \
      -H 'Content-Type: application/json' \
      -d "{\"msgtype\":\"text\",\"text\":{\"content\":\"ğŸ‰ 6GPUç»ˆæä¼˜åŒ–ä»»åŠ¡ $EXPERIMENT_NAME åœ¨ $(hostname) æˆåŠŸå®Œæˆäº $(date)\"}}"
else
    curl -s 'https://oapi.dingtalk.com/robot/send?access_token=979c5387ce734aea95b5b368629c8a412284dc2f52674dd0291eab5f154fb70a' \
      -H 'Content-Type: application/json' \
      -d "{\"msgtype\":\"text\",\"text\":{\"content\":\"âŒ 6GPUä»»åŠ¡ $EXPERIMENT_NAME åœ¨ $(hostname) å¤±è´¥äº $(date)\"}}"
fi

echo "è„šæœ¬æ‰§è¡Œå®Œæ¯•"
